## Resource for Learning about LLMs

### LLM in general

- [mlabonne's Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks](https://github.com/mlabonne/llm-course)
- [The Novice's LLM Training Guide written by Alpin](https://rentry.org/llm-training#the-basics)
- [Benedict Neo's Roadmap to Learn AI in 2024](https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16)


### Finetuning techniques
- [Akshay Pachaar's Understanding LoRA: Low-rank Adaption of Large Language Models](https://mlspring.beehiiv.com/p/understanding-lora-lowrank-adaption-large-language-models)
- [Ali Mobarekati's Fine-Tuning Mistral 7b in Google Colab with QLoRA (complete guide)](https://medium.com/@codersama/fine-tuning-mistral-7b-in-google-colab-with-qlora-complete-guide-60e12d437cca)
- [Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms) + [Source code](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)
- [Supervised fine-tuning (SFT) of an LLM](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)
- [Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA](https://medium.com/@dassum/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
- [Fine-tune Falcon-7B on Your GPU with TRL and QLoRa](https://medium.com/@bnjmn_marie/fine-tune-falcon-7b-on-your-gpu-with-trl-and-qlora-4490fadc3fbb)
- [Mistral 7B: Recipes for Fine-tuning and Quantization on Your Computer](https://medium.com/towards-data-science/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77)
- [Don't Merge Your LoRA Adapter Into a 4-bit LLM](https://kaitchup.substack.com/p/dont-merge-your-lora-adapter-into?source=post_page-----2216ffcdc27b--------------------------------)
- [Finetune Mistral](https://www.kaggle.com/code/simonstorf/finetune-mistral)
- [LLM Instruction Finetuning + WandB](https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb)

### Quantization techniques
- [Siddharth vij's LLM Quantization | GPTQ | QAT | AWQ | GGUF | GGML | PTQ ](https://medium.com/@siddharth.vij10/llm-quantization-gptq-qat-awq-gguf-ggml-ptq-2e172cd1b3b5)
- [Maarten Grootendorst's Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)](https://www.youtube.com/watch?app=desktop&v=mNE_d-C82lI&embeds_referring_euri=https%3A%2F%2Fmaartengrootendorst.substack.com%2F&feature=emb_imp_woyt)

### RAG tutorial
- [Building long context RAG with RAPTOR from scratch](https://youtu.be/jbGchdTL7d0?si=QOGwTfPiIF3e-Rom)
- [Advanced RAG series: Indexing](https://div.beehiiv.com/p/advanced-rag-series-indexing)
- [Local Retrieval Augmented Generation (RAG) from Scratch (big tutorial)](https://www.youtube.com/watch?v=qN_2fnOPY-M)

### To-Read articles
- [Don't Merge Your LoRA Adapter Into a 4-bit LLM](https://kaitchup.substack.com/p/dont-merge-your-lora-adapter-into?source=post_page-----2216ffcdc27b--------------------------------)
- [LoRA: Load and Merge Your Adapters with Care](https://medium.com/@bnjmn_marie/lora-load-and-merge-your-adapters-with-care-3204119f0426)
- [Compression Techniques for LLMs](https://medium.com/@bnjmn_marie/compression-techniques-for-llms-4eba6a6e622c)
- [GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs â€” Examples with Llama 2](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)

### To-Read Publication
- [T-RAG: LESSONS FROM THE LLM TRENCHES](https://arxiv.org/pdf/2402.07483.pdf)
- [LoRA+](https://arxiv.org/abs/2402.12354)

### Source code about data preprocessing
- [Construct dataset](https://github.com/avisoori-databricks/Tuning-the-Finetuning/blob/main/Step%200%20Constructing%20the%20dataset.py)

### Good community
- [RedditLocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)
