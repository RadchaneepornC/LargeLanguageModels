## Resource for Learning about LLMs

### LLM in general

- [mlabonne's Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks](https://github.com/mlabonne/llm-course)
- [The Novice's LLM Training Guide written by Alpin](https://rentry.org/llm-training#the-basics)
- [Benedict Neo's Roadmap to Learn AI in 2024](https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16)
- [Thomas@HuggingFace's A little guide to building Large Language Models in 2024](https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit#slide=id.p)
- [Exploring Large Language Models](https://medium.com/data-science-engineering/exploring-large-language-models-8fed99a5a139)

### LLM - All about theory
- [Training and fine-tuning large language models](https://www.borealisai.com/research-blogs/training-and-fine-tuning-large-language-models/)

### Finetuning techniques
- [Akshay Pachaar's Understanding LoRA: Low-rank Adaption of Large Language Models](https://mlspring.beehiiv.com/p/understanding-lora-lowrank-adaption-large-language-models)
- [Ali Mobarekati's Fine-Tuning Mistral 7b in Google Colab with QLoRA (complete guide)](https://medium.com/@codersama/fine-tuning-mistral-7b-in-google-colab-with-qlora-complete-guide-60e12d437cca)
- [Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms) + [Source code](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)
- [Supervised fine-tuning (SFT) of an LLM](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)
- [Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA](https://medium.com/@dassum/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
- [Fine-tune Falcon-7B on Your GPU with TRL and QLoRa](https://medium.com/@bnjmn_marie/fine-tune-falcon-7b-on-your-gpu-with-trl-and-qlora-4490fadc3fbb)
- [Mistral 7B: Recipes for Fine-tuning and Quantization on Your Computer](https://medium.com/towards-data-science/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77)
- [Don't Merge Your LoRA Adapter Into a 4-bit LLM](https://kaitchup.substack.com/p/dont-merge-your-lora-adapter-into?source=post_page-----2216ffcdc27b--------------------------------)
- [Finetune Mistral](https://www.kaggle.com/code/simonstorf/finetune-mistral)
- [LLM Instruction Finetuning + WandB](https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb)
- [Fine Tuning LLM: Parameter Efficient Fine Tuning (PEFT) â€” LoRA & QLoRA â€” Part 2](https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f)
- [Fine-tune Llama 2 for sentiment analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis/log)
- [Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.](https://ai.plainenglish.io/fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9)
- [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
  

### Loss function LLMs used when finetuning
- [Language Model Training and Inference: From Concept to Code](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#Â§understanding-next-token-prediction)
- [What is Loss Function: LLMs Explained](https://www.chatgptguide.ai/2024/02/29/what-is-loss-function-llms-explained/)
- [Cross Entropy in Large Language Models (LLMs)](https://medium.com/ai-assimilating-intelligence/cross-entropy-in-large-language-models-llms-4f1c842b5fca)
- [How do Large Language Models learn?](https://medium.com/@jeraldteokj/visualising-loss-calculation-in-large-language-models-1af410a9d73d)

### Evaluation metric
- [LLMs â€” Fine-tuning and Model Evaluation](https://ritikjain51.medium.com/llms-fine-tuning-and-evaluation-f019515b1c67)
- [ROUGE Metrics: Evaluating Summaries in Large Language Models.](https://pub.towardsai.net/rouge-metrics-evaluating-summaries-in-large-language-models-d200ee7ca0e6)
- [Customized Evaluation Metrics with Hugging Face Trainer](https://medium.com/@rakeshrajpurohit/customized-evaluation-metrics-with-hugging-face-trainer-3ff00d936f99)
- [Metrics for fine-tuning large language models in Autopilot](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-metrics.html)

### LLM hyperparameter tuning
- [Tuning parameters to train LLMs (Large Language Models)](https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971)

### Inference 
- [LLM Inference on multiple GPUs with ðŸ¤— Accelerate](https://medium.com/@geronimo7/llms-multi-gpu-inference-with-accelerate-5a8333e4c5db)

### Evaluation & Deployment
- [tensorrtllm_backend](https://github.com/triton-inference-server/tensorrtllm_backend)

### Quantization techniques
- [Siddharth vij's LLM Quantization | GPTQ | QAT | AWQ | GGUF | GGML | PTQ ](https://medium.com/@siddharth.vij10/llm-quantization-gptq-qat-awq-gguf-ggml-ptq-2e172cd1b3b5)
- [Maarten Grootendorst's Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)](https://www.youtube.com/watch?app=desktop&v=mNE_d-C82lI&embeds_referring_euri=https%3A%2F%2Fmaartengrootendorst.substack.com%2F&feature=emb_imp_woyt)

### RAG tutorial
- [Building long context RAG with RAPTOR from scratch](https://youtu.be/jbGchdTL7d0?si=QOGwTfPiIF3e-Rom)
- [Advanced RAG series: Indexing](https://div.beehiiv.com/p/advanced-rag-series-indexing)
- [Local Retrieval Augmented Generation (RAG) from Scratch (big tutorial)](https://www.youtube.com/watch?v=qN_2fnOPY-M)
- [hymie122 RAG-Survey](https://github.com/hymie122/rag-survey?fbclid=IwAR3aEDP6gqky7SBR3Whi-7PiDmcybWGAhkLkPDtBocjX3B57KcMbZX1Qax4)
- [RAG for long context LLMs](https://youtu.be/SsHUNfhF32s?si=H9SE2IyGrOrsXg63)

### To-Read articles
- [Don't Merge Your LoRA Adapter Into a 4-bit LLM](https://kaitchup.substack.com/p/dont-merge-your-lora-adapter-into?source=post_page-----2216ffcdc27b--------------------------------)
- [LoRA: Load and Merge Your Adapters with Care](https://medium.com/@bnjmn_marie/lora-load-and-merge-your-adapters-with-care-3204119f0426)
- [Compression Techniques for LLMs](https://medium.com/@bnjmn_marie/compression-techniques-for-llms-4eba6a6e622c)
- [GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs â€” Examples with Llama 2](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)

### To-Read Publication
- [T-RAG: LESSONS FROM THE LLM TRENCHES](https://arxiv.org/pdf/2402.07483.pdf)
- [LoRA+](https://arxiv.org/abs/2402.12354)
- [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) <-Finetuning + RAG
- https://www.linkedin.com/pulse/beyond-boundaries-human-like-approach-question-over-sources-lehmann-dhtne/?trackingId=XfiH5qabfTtsXuHRy%2BjA6g%3D%3D

### Source code about data preprocessing
- [Construct dataset](https://github.com/avisoori-databricks/Tuning-the-Finetuning/blob/main/Step%200%20Constructing%20the%20dataset.py)

### Good playlist
- [PakapongZa's LLM is all you need](http://www.youtube.com/playlist?list=PL-oZj9xqG7m8yOZdo3ipcQ2-p92Y5qG9Q)
### Good community
- [RedditLocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)

### Interesting usecases tutorials
- [Q/A chatbot with LLMs + Harry Potter](https://www.kaggle.com/code/hinepo/q-a-chatbot-with-llms-harry-potter)

### Generative AI paper (for futher literature review)
- [The Potential of Generative Artificial Intelligence Across Disciplines: Perspectives and Future Directions](https://www.tandfonline.com/doi/full/10.1080/08874417.2023.2261010?fbclid=IwAR1w9QK4iNqCduK9g6l61OpQRMlQLqG2hmPb1sM5QYsdLrge0JAqBwbCDVw)
